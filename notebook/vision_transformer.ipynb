{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vision Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tifffile\n",
    "from PIL import Image\n",
    "\n",
    "df =\n",
    "print(df.shape)\n",
    "\n",
    "with tifffile.TiffFile(df.FileName_Merged[0]) as tif:\n",
    "    multichannel_img = tif.asarray().astype(np.float32)\n",
    "    print(multichannel_img.shape)\n",
    "    multichannel_img = multichannel_img.transpose(1,2,0)  \n",
    "    print(multichannel_img.shape)\n",
    "    rgb_image = create_rgb_image(multichannel_img)\n",
    "   \n",
    "    print('reshape dimensions')\n",
    "    rgb_image = rgb_image.transpose(2, 1, 0)  # Transpose to (H, W, C) format\n",
    "    rgb_image = Image.fromarray(rgb_image, mode='RGB')\n",
    "    # print(rgb_image.shape)\n",
    "    print(rgb_image.mode)\n",
    "\n",
    "# plt.imshow(rgb_image)\n",
    "# plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Patch Embedding"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "Block contains a iteration of Attention\n",
    "embedding dim == number of features in the arch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "## sample input ##\n",
    "print('input image shape:', multichannel_img.shape) ## would be 224 x 224 for vit\n",
    "im = multichannel_img.transpose(2, 1, 0) \n",
    "print('original numpy image dim:', im.shape)\n",
    "tens = torch.tensor(im)\n",
    "print('original tensor image dim:', tens.size())\n",
    "\n",
    "\n",
    "\n",
    "## Create a projection layer ##\n",
    "patch_size = 16\n",
    "num_patches = (1080 // patch_size)**2 # (covers the area of the image)\n",
    "print('Total number of patches created:', num_patches)\n",
    "\n",
    "embed_dim = 768 #385 # embedding dimension, can be 768 for ViT-B/16\n",
    "proj = nn.Conv2d(4, embed_dim, kernel_size=patch_size, stride=patch_size, bias=False) # extract non-overlapping patches of 16x16 pixels\n",
    "proj.cuda()\n",
    "tens = tens.unsqueeze(0).cuda()  \n",
    "tens = tens.float() \n",
    "patch_emnedding = proj(tens)\n",
    "print('patch embedding dim: (batch, channel, height, width)', patch_emnedding.shape)\n",
    "print('flattened patch embedding dim: (batch, channels,length)', patch_emnedding.flatten(2).shape)\n",
    "print('transposed patch embedding dim: (batch, channels, length)', patch_emnedding.flatten(2).transpose(1,2).shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi Head Attention (MHA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_attn_heads = 12 # number of attention heads\n",
    "attn_head_dim = embed_dim//number_attn_heads # dimension of each head (dim of vector it opperats on)\n",
    "print('attention head dimension:', attn_head_dim)\n",
    "\n",
    "attn_scale = attn_head_dim ** -0.5 # scaling factor for attention scores\n",
    "\n",
    "print('embed_dim:', embed_dim)\n",
    "qkv = nn.Linear(embed_dim, embed_dim * 3, bias=False) # query, key, value projection\n",
    "\n",
    "attn_drop_layer = nn.Dropout(0.1) # dropout layer for attention scores  = 0.\n",
    "\n",
    "proj = nn.Linear(embed_dim, embed_dim) # final linear projection after attention = 0.\n",
    "proj = proj.cuda()\n",
    "proj_drop = nn.Dropout(p=0.1) # dropout layer after projection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## create query, key , value projection ##\n",
    "\n",
    "x = patch_emnedding.flatten(2).transpose(1,2)\n",
    "x2 = patch_emnedding.flatten(2).transpose(1,2)\n",
    "B,N,C = x.shape # batch, number of patches, embedding dimension\n",
    "print('B (batch):', B, 'N (# patches):', N, 'C (emb_dimension):', C)\n",
    "qkv.cuda()\n",
    "\n",
    "qkv_vecs = qkv(x)\n",
    "print('query, key, value projection (batch, # patches , 3xembed_dim):', qkv_vecs.shape) # apply linear projection to the input tensor\n",
    "\n",
    "qkv_vecs_distributed = qkv_vecs.reshape(B, N, 3, number_attn_heads, attn_head_dim)\n",
    "print('qkv projection reshaped: (batch, # patches, 3 \"vectors\", across x attention heads, with new dimension y)',qkv_vecs_distributed.shape) # reshape to (batch, num_patches, 3, num_heads, head_dim)\n",
    "\n",
    "qkv_vecs_dist_reord = qkv_vecs_distributed.permute(2, 0, 3, 1, 4)\n",
    "print('qkv projection permuted: (3 \"vectors\", batch, across x attention heads, # patches, with new dimension y)', qkv_vecs_dist_reord.shape) # permute to (3, batch, num_heads, num_patches, head_dim)\n",
    "\n",
    "q, k, v = qkv_vecs_dist_reord[0], qkv_vecs_dist_reord[1], qkv_vecs_dist_reord[2]\n",
    "print('query shape:', q.shape, 'key shape:', k.shape, 'value shape:', v.shape) # query, key, value shapes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## calculate attention on the q,k vectors ##\n",
    "# q matrix multiplication (k) q:(RxC)(CxR):k -> RXR\n",
    "attn = q @ k.transpose(-1,-2)\n",
    "print('attention score matrx (batch, # attention heads, number query patches, number key patches): ',attn.shape)\n",
    "# print(attn[0,0,0:5,0:5]) # print first 5x5 matrix of attention scores\n",
    "# scale the attention scores\n",
    "attn = attn * attn_scale\n",
    "# print(attn[0,0,0:5,0:5]) # print first 5x5 matrix of scaled attention scores\n",
    "\n",
    "attn = attn.softmax(dim=-1) # apply softmax to the attention scores\n",
    "print('scaled attention score matrix (batch, # attention heads, number query patches, number key patches): ', attn.shape)\n",
    "# print(attn[0,0,0:5,0:5]) # print first 5x5 matrix of attention scores after softmax \n",
    "\n",
    "# apply dropout to attention scores (regulization to prevent overfitting)\n",
    "attn = attn_drop_layer(attn)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### FINAL patch EMBEDDING ### across all patches bassed on attention scores ## CONTEXT RICH embedding for ea patch###\n",
    "x = (attn @ v).transpose(1,2).reshape(B, N, C) # reshape to (batch, num_patches, embed_dim) # apply attention to the value vectors (!!!!!acts as concatination of ea attention head output!!!!)\n",
    "print('x shape after attention (batch, # patches, embed_dim) CONTEXT RICH EMBEDDINGS for ea patch:\\n', x.shape) # x is the output of the attention layer\n",
    "\n",
    "\n",
    "\n",
    "### Used to mix the information from all patches accross all attention heads ###\n",
    "x = proj(x) # apply final linear projection \n",
    "print('x shape after final projection (batch, # patches, embed_dim):', x.shape) # x is the output of the attention layer after final projection\n",
    "# print(x[0,0:5,:]) # print first 5 patches of the output of the attention layer after final projection\n",
    "x = proj_drop(x) # apply dropout to the output of the attention layer\n",
    "# print('x shape after dropout (batch, # patches, embed_dim):', x.shape) # x is the output of the attention layer after dropout\n",
    "# print(x[0,0:5,:]) # print first 5 patches of the output of the attention layer after dropout\n",
    "\n",
    "# print(f' input:{x[0,0:5,:]},attention score: {attn[0,0,0:5,0:5]}, query: {q}, key: {k}, value: {v}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(attn @ v).shape # apply attention to the value vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(attn @ v).transpose(1,2).reshape(B, N, C).shape # reshape to (batch, num_patches, embed_dim) concatinates embeddings from all attention heads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DropPath"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "Regulization mechanisim also known as Stochastic Depth, where each block is randomly dropped during training with a probability p. \n",
    "This helps to prevent overfitting and allows for deeper networks.\n",
    "Acts as like dropout but at the block level and not just the individual elements.\n",
    "prevents overfitting by randomly dropping entire blocks of the network during training.\n",
    "\n",
    "each sample gets its own dropout decision, the same mask value (0 or 1) \n",
    "is shared across all other dimensions (across all tokens and embedding channels for a smaple).\n",
    "\n",
    "example:\n",
    "So, for x.shape = [1, 4489, 768]:\n",
    "The mask shape is [1, 1, 1]\n",
    "\n",
    "So each sample in the batch gets a scalar 0 or 1\n",
    "\n",
    "If it's 0, then the entire [1, 4489, 768] tensor is zeroed\n",
    "\n",
    "If it's 1, then it passes through (scaled by 1 / keep_prob)\n",
    "\n",
    "So:\n",
    "→ If the mask is 0, all 4489 tokens and their 768-dim embeddings are zeroed for that residual branch.\n",
    "→ Nothing gets dropped within a sample — it's all or nothing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_path_rate = 0.1\n",
    "drop_path_lst = [x.item() for x in torch.linspace(0, drop_path_rate, 12)]\n",
    "drop_path_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_prob = drop_path_lst[2]\n",
    "drop_probability = drop_prob\n",
    "\n",
    "training = False # set to True if you want to apply dropout, False if you want to use the model for inference\n",
    "\n",
    "if drop_probability == 0. or not training:\n",
    "    x = x\n",
    "\n",
    "\n",
    "#### creates the mask to randomly drop patches  ####\n",
    "keep_prob = 1. - drop_probability\n",
    "print(keep_prob) # this is the probability of keeping the neuron active\n",
    "\n",
    "shape = (x.shape[0],) + (1,) * (x.ndim-1) # this is the shape of the tensor after dropout \n",
    "print(shape) # shape of the tensor after dropout, (batch, 1, 1) for a single batch\n",
    "\n",
    "rnd_tensor = keep_prob + torch.rand(shape, dtype=x.dtype, device=x.device) # this is the random tensor used for dropout (mask)\n",
    "print(rnd_tensor.shape) # shape of the random tensor used for dropout, (batch, 1, 1) for a single batch\n",
    "\n",
    "print(rnd_tensor)\n",
    "\n",
    "rnd_tensor.floor_() # modifies the values in-place of the original tensor ### (binarizes the tensor) ###\n",
    "print(rnd_tensor) # this is the random tensor used for dropout after applying floor operation, it will be 0 or 1\n",
    "print(x.shape)\n",
    "print('\\n')\n",
    "####\n",
    "\n",
    "# patch embeddings #\n",
    "out = x.div(keep_prob)*rnd_tensor # this is the output of the dropout layer, it will be 0 or the original value divided by the keep probability\n",
    "\n",
    "print(out.shape) # shape of the output of the dropout layer, (batch, # patches, embed_dim) for a single batcho\n",
    "\n",
    "print(out[0,0:5,:]) # print first 5 patches of the output of the dropout layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnd_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.div(0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.Identity()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Residual Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resd_x = x+out\n",
    "resd_x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NormLayer"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "For each patch:\n",
    "Computes the mean and standard deviation of the embed_dim values\n",
    "Subtracts the mean and divides by the standard deviation\n",
    "Applies learnable scale and shift (γ and β)\n",
    "\n",
    "Stabilizes training\n",
    "Reduces internal covariate shift\n",
    "Helps the transformer learn better feature representations by ensuring consistent scaling across layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_layer = nn.LayerNorm(embed_dim) # create a layer normalization layer\n",
    "norm_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# applied before attention\n",
    "\n",
    "norm_layer = norm_layer.cuda() # move the layer normalization layer to the GPU\n",
    "# norm_layer(x2)\n",
    "norm_layer(resd_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x2.shape)\n",
    "(x2 - torch.mean(x2,dim=2,keepdim=True))/torch.std(x2,dim=2,keepdim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_x = norm_layer(x)\n",
    "print(norm_x.shape)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "LayerNorm uses Welford's algorithm or other numerically stable variants under the hood\n",
    "normalizes each token (i.e., across embed_dim), but with learnable parameters:\n",
    "weight (γ) and bias (β)\n",
    "Initialized to γ = 1, β = 0 by default\n",
    "\n",
    "Your manual method uses mean() and std() directly, which are less numerically stable when working with float32 precision\n",
    "\n",
    "This leads to very small numerical differences (on the order of 1e-4 or less), which is exactly what you're seeing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "After self-attention mixes information across tokens, the MLP (or feed-forward network, FFN) \n",
    "mixes information within each token's feature vector — meaning it works along the channel dimension,not across patches.\n",
    "\n",
    "Multi-Head Self-Attention (MHSA)\n",
    "– Mixes information between tokens (e.g., between atten heads in ViT)\n",
    "\n",
    "MLP / FFN\n",
    "– Mixes information within each token's embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_f = None\n",
    "in_f = embed_dim\n",
    "hidden_f = int(embed_dim*4)\n",
    "out_feat = out_f or in_f\n",
    "\n",
    "# Higer dimension projection\n",
    "fc1 = nn.Linear(in_f, hidden_f)\n",
    "fc1 = fc1.cuda()\n",
    "act_layer = nn.GELU()\n",
    "# project back to original dimension after\n",
    "fc2 = nn.Linear(hidden_f, out_feat)\n",
    "fc2 = fc2.cuda()\n",
    "drop = nn.Dropout(0.) # dropout layer after the final linear projection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Mixing information between embeddings after mixing across heads ##\n",
    "mlpx = fc1(x)\n",
    "mlpx = act_layer(mlpx) # apply activation function\n",
    "mlpx = drop(mlpx) # apply dropout\n",
    "mlpx = fc2(mlpx)\n",
    "mlpx = drop(mlpx) # apply dropout again\n",
    "print(mlpx.shape) # shape of the output of the mlpx layer, (batch, # patches, embed_dim) for a single batch\n",
    "mlpx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_x[:,0].shape"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
